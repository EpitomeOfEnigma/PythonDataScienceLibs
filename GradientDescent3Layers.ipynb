{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation application for beginners in ML\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) #INPUT ROWS TO THE NEURAL NETWORK\n",
    "#y = np.asarray([ [0,1,1,0] ]).T #OUTPUT COLUMN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Problem: predicting the output column from given input rows\n",
    "#Actual problem: measuring statistics between input and output to make a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### derived from quora answers at \n",
    "##### https://www.quora.com/What-is-the-sigmoid-function-and-what-is-its-use-in-machine-learnings-neural-networks-How-about-the-sigmoid-derivative-function\n",
    "\n",
    "### Points to consider when choosing activation function\n",
    "# continuity of the function\n",
    "# computational power to process all neurons of the network\n",
    "# type of desired output (logistic/continuous variables or classification/categorical data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####### Sigmoid activation function\n",
    "\n",
    "### Output boundaries of sigmoid: (0,1)\n",
    "### Derivative of sigmoid: sigmoid'(x) = sigmoid(x)*(1-sigmoid(x))\n",
    "### Usage\n",
    "# 1) transforms linear inputs to non-linear outputs, so that the changes in output forms a smooth curve\n",
    "# 2) bounds output between (0,1) so that it can be interpreted as probability\n",
    "# 3) makes computations easier than when using quadratic, cubic functions (these give smoother output change curve too) \n",
    "#\t because exponential functions are similar to handle mathematically \n",
    "# 4) gives real-valued and differentiable output predictions (needed to find gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x, deriv = False):\n",
    "\tif(deriv==True):\n",
    "\t\treturn x*(1-x) #derivative of sigmoid function\n",
    "\treturn 1/(1+np.exp(-x)) #sigmoid function definition mathematically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input dataset \n",
    "X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ])\n",
    "\n",
    "#output dataset\n",
    "y = np.asarray([ [0,1,1,0] ]).T "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Seeding random numbers to make a calculation will still make the numbers randomly distributed, but in exactly the same way each time you train. This makes it easier to see how your changes affect the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deterministic \n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initialize weights randomly with mean 0\n",
    "# array_var = (b-a) * np.random.random((b,a)) - 1\n",
    "# returns random floats in  interval [b,a)\n",
    "\n",
    "This is our weight matrix for this neural network. It's called \"syn0\" to imply \"synapse zero\". Since we only have 3 layers (input, hidden layer and output), we only need two matrices of weights to connect them. The dimensions are (3,4) and (4,1) because we have 3 inputs and 4 outputs, and 4 inputs and 1 output, respectively. Thus, we want to connect every node in l0 to every node in l1, which requires a matrices of dimensionality (3,4) and (4,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0 = 2* np.random.random((3,4)) - 1 \n",
    "syn1 = 2* np.random.random((4,1)) - 1 \n",
    "#first layer of weights, synapse 0, connecting l0 to l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.49641003190272537\n",
      "Error:0.008584525653247159\n",
      "Error:0.0057894598625078085\n",
      "Error:0.004629176776769985\n",
      "Error:0.003958765280273649\n",
      "Error:0.0035101225678616766\n"
     ]
    }
   ],
   "source": [
    "#actual training\n",
    "#full batch training or batch gradient descent training\n",
    "for iter in range(60000):\n",
    "\n",
    "\t#forward propagation\n",
    "\tl0 = X \t\t# first layer of Network, specified by input data\n",
    "\tl1 = nonlin(np.dot(l0, syn0)) \t#second layer of network, known as hidden layer\n",
    "\tl2 = nonlin(np.dot(l1, syn1))\n",
    "\n",
    "\t#how much did we miss?\n",
    "\tl2_error = y - l2\n",
    "    \n",
    "\tif((iter% 10000) == 0) :\n",
    "\t\tprint(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "\tl2_delta = l2_error * nonlin(l2, deriv=True)\n",
    "   \n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "\tl1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "\t# in what direction is the target l1?\n",
    "\t# were we really sure? if so, don't change too much.\n",
    "\tl1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "\t#update weights \n",
    "\tsyn1 += l1.T.dot(l2_delta)\n",
    "\tsyn0 += l0.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after Training: \n",
      "[[7.26191199e-01 1.16411907e-01 9.26183940e-01 9.97110310e-01]\n",
      " [1.66762801e-01 3.92990161e-04 1.66519465e-02 8.96576847e-01]\n",
      " [9.96229372e-01 8.95211165e-01 2.23120442e-02 8.38385421e-01]\n",
      " [9.52239003e-01 2.48589483e-02 3.07990327e-05 1.15301801e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output after Training: \")\n",
    "print(l1)\n",
    "\n",
    "#Output After Training:\n",
    "#[[ 0.00966449]\n",
    "# [ 0.00786506]\n",
    "# [ 0.99358898]\n",
    "# [ 0.99211957]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
