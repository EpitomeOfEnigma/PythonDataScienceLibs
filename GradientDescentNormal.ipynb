{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation application for beginners in ML\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) #INPUT ROWS TO THE NEURAL NETWORK\n",
    "#y = np.asarray([ [0,1,1,0] ]).T #OUTPUT COLUMN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Problem: predicting the output column from given input rows\n",
    "#Actual problem: measuring statistics between input and output to make a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### derived from quora answers at \n",
    "##### https://www.quora.com/What-is-the-sigmoid-function-and-what-is-its-use-in-machine-learnings-neural-networks-How-about-the-sigmoid-derivative-function\n",
    "\n",
    "### Points to consider when choosing activation function\n",
    "# continuity of the function\n",
    "# computational power to process all neurons of the network\n",
    "# type of desired output (logistic/continuous variables or classification/categorical data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####### Sigmoid activation function\n",
    "\n",
    "### Output boundaries of sigmoid: (0,1)\n",
    "### Derivative of sigmoid: sigmoid'(x) = sigmoid(x)*(1-sigmoid(x))\n",
    "### Usage\n",
    "# 1) transforms linear inputs to non-linear outputs, so that the changes in output forms a smooth curve\n",
    "# 2) bounds output between (0,1) so that it can be interpreted as probability\n",
    "# 3) makes computations easier than when using quadratic, cubic functions (these give smoother output change curve too) \n",
    "#\t because exponential functions are similar to handle mathematically \n",
    "# 4) gives real-valued and differentiable output predictions (needed to find gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x, deriv = False):\n",
    "\tif(deriv==True):\n",
    "\t\treturn x*(1-x) #derivative of sigmoid function\n",
    "\treturn 1/(1+np.exp(-x)) #sigmoid function definition mathematically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input dataset \n",
    "X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ])\n",
    "\n",
    "#output dataset\n",
    "y = np.asarray([ [0,1,1,0] ]).T "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Seeding random numbers to make a calculation will still make the numbers randomly distributed, but in exactly the same way each time you train. This makes it easier to see how your changes affect the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deterministic \n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initialize weights randomly with mean 0\n",
    "# array_var = (b-a) * np.random.random((b,a)) - 1\n",
    "# returns random floats in  interval [b,a)\n",
    "\n",
    "This is our weight matrix for this neural network. It's called \"syn0\" to imply \"synapse zero\". Since we only have 2 layers (input and output), we only need one matrix of weights to connect them. Its dimension is (3,1) because we have 3 inputs and 1 output. Another way of looking at it is that l0 is of size 3 and l1 is of size 1. Thus, we want to connect every node in l0 to every node in l1, which requires a matrix of dimensionality (3,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0 = 2* np.random.random((3,1)) - 1 \n",
    "#first layer of weights, synapse 0, connecting l0 to l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual training\n",
    "#full batch training or batch gradient descent training\n",
    "for iter in range(10000):\n",
    "\n",
    "\t#forward propagation\n",
    "\tl0 = X \t\t# first layer of Network, specified by input data\n",
    "\tl1 = nonlin(np.dot(l0, syn0)) \t#second layer of network, known as hidden layer\n",
    "\n",
    "\t#how much did we miss?\n",
    "\tl1_error = y - l1\n",
    "\n",
    "\t#multiply how much we missed by the\n",
    "\t#slope of the sigmoid at the values in l1\n",
    "\n",
    "\tl1_delta = l1_error * nonlin(l1, True)\n",
    "\n",
    "\t#update weights \n",
    "\tsyn0 += np.dot(l0.T, l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after Training: \n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output after Training: \")\n",
    "print(l1)\n",
    "\n",
    "#Output After Training:\n",
    "#[[ 0.00966449]\n",
    "# [ 0.00786506]\n",
    "# [ 0.99358898]\n",
    "# [ 0.99211957]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
